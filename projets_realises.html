<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Projets Réalisés</title>
  <link rel="stylesheet" href="style.css">

  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-VDNMV1EVS3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-VDNMV1EVS3');
</script>
  
</head>
<body>
  <header>
    <h1>Projets Réalisés</h1>
    <nav>
       <a href="index.html">Accueil</a>
      <a href="competences.html">Compétences</a>
      <a href="experiences.html">Expériences</a>
      <a href="formation.html">Formation</a>
      <a href="projets_realises.html">Projets</a>
      <a href="certifications.html">Certifications</a>
      <a href="recommandations.html">Recommandations</a>
      <a href="contact.html">Contact</a>
    </nav>
  </header>
  <main>

  <section class="project">
  <h2>Développement d'une application carbone</h2>
  <p>
    <strong>Contexte :</strong> Développement d’un outil innovant dans Revit pour estimer l’empreinte carbone des projets architecturaux dès les phases d’études préliminaires et d’avant-projet.  
    Cet outil permet de calculer une estimation globale du bilan carbone à partir des données disponibles dans le modèle Revit.  
    Il aide les décideurs à comprendre rapidement l’impact environnemental potentiel de leurs constructions et à prendre des décisions plus durables.
  </p>

  <p>
    L’outil a été conçu pour être facilement intégrable et simple d’utilisation. Il permet également de générer des rapports automatisés ainsi que des vues 3D colorées pour visualiser l’impact carbone de chaque composant du bâtiment.  
    j'ai  joué un rôle clé dans son développement, notamment dans la programmation en Python et l’intégration technique avec Revit.  
    Grâce à ma contribution, l’outil sera déployé chez AREP à l’été 2025.
  </p>

  <p><strong>Tâches effectuées :</strong> Développement d’une application sur Revit (Python/C#), génération automatique du calcul d’empreinte carbone, visualisation interactive avec Dash & Power BI, assistance à la prise de décision écologique.</p>

  <p><strong>Outils :</strong> Python, Revit, C#, Dash, Power BI</p>
</section>

  

    <section class="project">
      <h2>Prévision de la consommation d’énergie électrique</h2>
      <p><strong>Contexte :</strong> Analyse de séries temporelles pour prévoir la consommation électrique et détecter anomalies/saisonnalité.</p>
      <p><strong>Tâches effectuées :</strong> Analyse SARIMA/SARIMAX de données (2012-2021), sélection automatique des meilleurs paramètres, prédictions.</p>
      <p><strong>Outils :</strong> Python, ARIMA, SARIMA, SARIMAX, matplotlib, seaborn, statsmodels</p>
    </section>


    <section class="project">
  <h2>Détection de fake news via Deep Learning</h2>
  <p><strong>Contexte :</strong> Lutte contre la désinformation diffusée sur les réseaux sociaux, notamment concernant la santé des dirigeants.</p>

  <p><strong>Tâches effectuées :</strong> Collecte de données (Facebook, Twitter, etc.), traitement en CSV, NLP, entraînement de modèles pour la détection automatique de fausses nouvelles.</p>

  <p>Pour ce projet, j’ai choisi de m’appuyer sur l’intelligence artificielle (IA), qui offre une solution prometteuse pour automatiser la détection des fake news. En utilisant des techniques avancées de traitement du langage naturel (NLP), j’ai récolté des données à partir de diverses sources telles que Facebook, Google, Twitter, la presse et YouTube.</p>

  <p>Par exemple, j’ai récupéré toutes les vidéos évoquant les fausses nouvelles sur la santé du président. Je me suis concentré sur les commentaires trouvés sur Google, j’ai collecté des tweets liés à ce sujet sur Twitter, rassemblé des articles de presse, et analysé les publications et commentaires sur Facebook.</p>

  <p>J’ai ensuite structuré mes données dans un fichier CSV comportant quatre colonnes :
    <ul>
      <li><strong>sources :</strong> origine des données (site web ou plateforme sociale),</li>
      <li><strong>commentaire :</strong> texte de la nouvelle ou du commentaire,</li>
      <li><strong>malade :</strong> identification du sujet (ici, la santé du président),</li>
      <li><strong>auteur :</strong> nom ou identifiant de l’auteur du contenu.</li>
    </ul>
  </p>

  <p>Après avoir préparé ces fichiers CSV, j’ai utilisé la bibliothèque Pandas en Python pour les concaténer en un seul fichier. Grâce à la fonction <code>concat()</code>, j’ai pu fusionner toutes les données en un fichier consolidé prêt à l’analyse.</p>

  <p>Ce fichier unique contient l’ensemble des données structurées. La colonne "sources" permet de retracer l’origine de chaque entrée, "commentaire" fournit le contenu textuel, "malade" permet d’identifier les fausses nouvelles liées à la santé, et "auteur" indique l’origine du contenu.</p>

  <p>Ce processus de collecte et de structuration des données est essentiel pour entraîner un système d’IA performant. Il facilite l’analyse des tendances, la détection des motifs récurrents dans les fausses nouvelles, et l’identification de sources peu fiables.</p>

  <p>En résumé, j’ai pris en charge l’intégralité de la collecte et de l’organisation des données, créant ainsi une base robuste pour mon projet de détection de fake news.</p>

  <p><strong>Outils :</strong> Python, stopwords, Logistic Regression, Random Forest, Matplotlib, Seaborn</p>
</section>




  <section class="project">
  <h2>Assistant IA – Jeux Olympiques 2024</h2>

  <p><strong>Contexte :</strong>  
  Développement d’un chatbot prédictif basé sur l’IA pour répondre à toutes les questions liées aux Jeux Olympiques de Paris 2024, notamment les classements, les performances des athlètes, et les prévisions de médailles.</p>

  <p><strong>Tâches effectuées :</strong>  
  J’ai conçu et testé plusieurs prompts spécialisés afin d’adapter les réponses du chatbot selon le contexte :  
  <ul>
    <li><code>JO_PROMPT</code> : Fournit les informations officielles sur le classement des JO 2024.</li>
    <li><code>JO_PROMPT2</code> : Présente les performances remarquables et les médaillés marquants des JO.</li>
    <li><code>JO_PROMPT3</code> : Prompt destiné à un assistant expert en sport. Il analyse les chances de médailles d’un pays ou d’un athlète en se basant sur les résultats précédents, les classements mondiaux, et les tendances observées, sans jamais donner de certitudes.</li>
  </ul>
  Ce système de prompts permet une réponse contextuelle, fiable et nuancée selon le type de requête utilisateur.</p>

  <p><strong>Outils :</strong> Python, OpenAI API, Streamlit</p>
</section>


  <section class="project">
  <h2>Extraction de données depuis des documents PDF</h2>

  <p><strong>Contexte :</strong>  
  L’objectif était de collecter des données à partir de documents PDF (avec ou sans couche texte), d’extraire automatiquement des informations clés et de les structurer dans un fichier CSV contenant les colonnes suivantes :  
  <code>"enterprise_name", "raison_d_etre", "objectifs_sociaux", "objectifs_environnementaux", "mission_nature"</code>.  
  Le projet s’appuie sur des techniques d’OCR, de traitement de texte et d’intelligence artificielle (modèle OpenAI GPT).</p>

  <p><strong>Tâches effectuées :</strong>  
    - Identification des PDF contenant une couche texte ou non à l’aide de <code>pdfplumber</code> ;<br>
    - Utilisation de <code>pdfplumber</code> pour les PDF textuels, et de <code>pytesseract</code> pour l’OCR des PDF scannés ;<br>
    - Développement d’une fonction de découpage du texte en fragments pour respecter les limites de tokens des modèles IA ;<br>
    - Création d’un système de requêtes structurées vers le modèle GPT pour extraire des informations spécifiques (nom d’entreprise, raison d’être, missions RSE, etc.) ;<br>
    - Implémentation d’une logique d’agrégation des résultats issus de plusieurs pages (chunks) ;<br>
    - Traitement automatisé d’un dossier complet de PDF, avec sauvegarde des résultats dans un fichier CSV ;<br>
    - Gestion des erreurs, vérification des réponses de l’API, gestion des cas d’échec avec fallback JSON sécurisé.</p>

  <p><strong>Outils :</strong> Python, pdfplumber, pytesseract, OpenAI API, CSV, JSON</p>
</section>

    
    <section class="project">
    <h2>Calcul de la performance commerciale, analyse de données et visualisation avec Power BI</h2>
    <p><strong>Contexte :</strong> Création d’un tableau de bord Power BI mettant en avant :
      <ul>
        <li>Les produits les plus performants</li>
        <li>Les pays ou clients les plus rentables</li>
        <li>Des indicateurs de succès clairs</li>
        <li>Des axes d'amélioration potentiels</li>
      </ul>
    </p>
    <p><strong>Tâches effectuées :</strong> Pour évaluer la performance, j’ai d’abord mis en place une table de dates. Ensuite, j’ai réalisé plusieurs opérations : création d’une colonne pour le montant total des ventes, définition d’une mesure pour le total des ventes, puis calcul des ventes des années précédentes afin de mesurer les performances des produits. J’ai également créé une mesure nommée "Ventes précédentes", permettant cette comparaison, ainsi qu’une mesure "Évolution des ventes" pour visualiser les tendances par produit.</p>
    <p><strong>Outils :</strong> Power BI, DAX, Power Query, mesures, colonnes calculées, Excel</p>
  </section>

  </main>
  <footer>
    <p>Souleymane Daffe  DATA SCIENTIS/ANALYST/DEV IA</p>
  </footer>
</body>
</html>
